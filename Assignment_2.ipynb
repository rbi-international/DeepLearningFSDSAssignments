{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b01dca6",
   "metadata": {},
   "source": [
    "## 1.\tDescribe the structure of an artificial neuron. How is it similar to a biological neuron? What are its main components?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5ea69c",
   "metadata": {},
   "source": [
    "**Artificial neuron**\n",
    "---\n",
    "ANNs are inspired by biological neurons found in cerebral cortex of our brain. It is convenient to visualize neurons as arranged in layers. In addition, it can be understood that neurons in the same layer behave in the same manner.The key factors in determining the behaviour of a neuron are its activation function and the pattern in determining the behaviour of a neuron are its activation function and the pattern of weighted connections over which it sends and receives signals.. Within each layer, neurons usually have the same activation function and the same pattern of connections to other neurons. However, based on the number of layers and the pattern of connections, the NN can be of different types.\n",
    "\n",
    "![ANN](https://static.javatpoint.com/tutorial/artificial-neural-network/images/artificial-neural-network3.png)\n",
    "\n",
    "**Biological Neuron**\n",
    "---\n",
    "In human brain, the grey matter consists of neuron cell bodies and dendrites; and the white matter (myelin) consists of axon tracts. The neuron receive signals and produce a response. \n",
    "\n",
    "![Biological neuron](https://cdn.codespeedy.com/wp-content/uploads/2019/05/neuron-structure.jpg)\n",
    "\n",
    "The branches to the left are the transmission channels for incoming information and are called 'dendrites'. Dendrites receive the signals at the contact regions with other cells called the *synapses*. Organelles in the body of the cell produce all the necessary chemicals for the continuos working of the neuron. The mitochondria, is the energy supply of the cell structure. The output signals are transmitted by the axon, of which each cell has at most one. The main components are:\n",
    "1. Dendrites\n",
    "2. Synapses\n",
    "3. Cell body/soma\n",
    "4. Axon\n",
    "\n",
    "Here, Information enters the nerve cell at the synaptic site on the dendrite, the processed information propagates action potentials leave the soma-dendrite complex to travel to the axon terminals. This same combination and formation of biological neurons were taken into consideration for the begining of ANNs creation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5442d6c",
   "metadata": {},
   "source": [
    "## 2.\tWhat are the different types of activation functions popularly used? Explain each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3285bfa",
   "metadata": {},
   "source": [
    "When we build our neural network, one of the design decisions that we will need to make is what activation function to use for our neuron's calculation. Activation functions are also referred to as transfer functions or nonlinearities because they transform the linear combination of a weighted sum into a nonlinear model. An activation function is placed at the end of each perceptron to decide whether to activate this neuron.\n",
    "\n",
    "**Different types of popular activation functions**\n",
    "\n",
    "1. **Linear transfer function**: A linear transfer function, also called as identity function, indicates that the function passes a signal through unchanged. In practical terms, the output will be equal to the input, which means we don't actually have an activation function. So no matter how many layers our neural network has, all it is doing is computing a linear activation or, at most, scaling the weighted average coming in. But it doesn't transform input into a nonlinear function.\n",
    "\n",
    "$$activation(z) = z = wx + b$$\n",
    "\n",
    "The composition of two linear functions is a linear function, so unless we throw a nonlinear activation function in our neural network, we are not computing any interesting functions no matter how deep we make our network. No learning here!\n",
    "\n",
    "![Linear activation function](https://miro.medium.com/max/1200/1*NOWmZpY6va1QWRpI6MPYwQ.png)\n",
    "\n",
    "The derivative of a linear function is constant: it does not depend on the input value. This means that every time we do a back propagation, the gradient will be the same. And this is a big problem: we are not really improving the error, since the gradient is pretty much the same. \n",
    "\n",
    "2. **Heavyside step function (binary classifier)**\n",
    "\n",
    "The Step function produces a binary output. It basically says that if the input x > 0, it fires (output y = 1) else it doesn't fire (output y = 0). It is mainly used in binary classification problems like true or false, spam or not spam, and pass or fail.\n",
    "\n",
    "![step function](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d8/Step_function.svg/1020px-Step_function.svg.png)\n",
    "\n",
    "$$output = \\left\\{\\begin{matrix}\n",
    "0, if w\\cdot x + b \\leq 0\n",
    " &  & \\\\ \n",
    "1, if w\\cdot x + b > 0\n",
    " &  & \n",
    "\\end{matrix}\\right.$$\n",
    "\n",
    "Step functions are commonly used in binary classification problems because they transform the input into zero or one.\n",
    "\n",
    "3. **Sigmoid/logistic functions**\n",
    "\n",
    "This is one of the most common activation functions. It is often used in binary classifier to predict the probability of a class when you have two classes. The sigmoid squishes all the values to a probability between 0 and 1, which reduces extreme values or outliers in the data without removing them. Sigmoid or logistic functions convert infinite continuous variables (range between - inf to + inf) into simple probabilities between 0 and 1. It is also called the S-shape curve because when plotted in a graph, it produces an S-shaped curve. While the step function is used to produce a discrete answer (pass or fail), sigmoid is used to produce the probability of passing and proability of failing.\n",
    "\n",
    "$$\\sigma (z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "![sigmoid](https://miro.medium.com/max/1200/1*a04iKNbchayCAJ7-0QlesA.png)\n",
    "\n",
    "4. **Softmax function**\n",
    "\n",
    "The softmax function is a generalization of the sigmoid function. It is used to obtain classification probabilities when we have more than two classes. It forces the outputs of a neural network to sum to 1 (for example, 0 < output < 1). A very common use case in deep learning problems is to predict a single class out of many options (more than two).\n",
    "\n",
    "$$\\sigma (x_{j}) = \\frac{e^{x_{j}}}{\\sum_{i}e^{x_{j}}}$$\n",
    "\n",
    "![softmax](https://www.bragitoff.com/wp-content/uploads/2021/12/Softmax-Activation.png)\n",
    "\n",
    "5. **Hyperbolic tangent function (tanh)**\n",
    "\n",
    "The hyperbolic tangent function is a shifted version of the sigmoid version. Instead of squeezing the signal values between 0 and 1, tanh squishes all values to the range -1 to 1. Tanh almost always works better than the sigmoid function in hidden layers because it has the effect of centering your data so that the mean of the data is close to zero rather than 0.5, which makes learning for the next layer a little bit easier:\n",
    "\n",
    "$$tanh(x) = \\frac{sinh(x)}{cosh(x)} =  \\frac{e ^ {x} - e^{-x}}{e ^ {x} + e^{-x}}$$\n",
    "\n",
    "![tanh](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_4.23.22_PM_dcuMBJl.png)\n",
    "\n",
    "One of the downsides of both sigmoid and tanh functions is that if (z) is very large or very small, then the gradient (or derivative of slope) of this function becomes very small (close to zero), which will slow down gradient descent. This is when the ReLU activation function provides a solution.\n",
    "\n",
    "6. **Rectified Linear Unit**\n",
    "\n",
    "The Rectified Linear Unit (ReLU) activation function activates a node only if the input is above zero. If the input is below zero, the output is always zero. But when the input is higher than zero, it has a linear relationship with the output variable. The ReLU function is represented as follows:\n",
    "\n",
    "$$f(x) = max(0, x)$$\n",
    "\n",
    "At the time of writing, ReLU is considered the state-of-the-art activation function because it works well in many different situations, and it tends to train better than sigmoid and tanh in hidden layers.\n",
    "\n",
    "\n",
    "![ReLU](https://pytorch.org/docs/stable/_images/ReLU.png)\n",
    "\n",
    "The ReLU function eliminates all negative values of the input by transforming them into zeros.\n",
    "$$ relu(x) = \\left\\{\\begin{matrix}\n",
    "0; if x < 0 \\\\\n",
    "1; if x \\geq 0\n",
    "\\end{matrix}\\right.$$\n",
    "\n",
    "7. **Leaky ReLU**\n",
    "\n",
    "One disadvantage of ReLU activation function is that the derivative is equal to to zero when (x) is negative. Leaky ReLU is a ReLU variation that tries to mitigate this issue. Instead of having the function be zero when x < 0, leaky ReLU introduces a small negative slope (around 0.01) when (x) is negative. It usually works better than the ReLU function, although it's not used as much in practice.\n",
    "$$f(x) = max(0.01x, x)$$\n",
    "\n",
    "![Leaky ReLU](https://qph.cf2.quoracdn.net/main-qimg-201f753cd667165eaa48b88a119aa1ef)\n",
    "\n",
    "$$ f(x) = \\left\\{\\begin{matrix}\n",
    "0.01(x)\\ for\\ x < 0 \\\\\n",
    "x\\ for\\ x \\geq 0\n",
    "\\end{matrix}\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ba72a0",
   "metadata": {},
   "source": [
    "## 3.1 Explain, in details, Rosenblattâ€™s perceptron model. How can a set of data be classified using a simple perceptron?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0effad3",
   "metadata": {},
   "source": [
    "Rosenblatt perceptron is a binary single neuron model. The inputs integration is implemented through the addition of the weighted inputs that have fixed weights obtained during the training stage. If the result of this addition is larger than a given threshold Î¸ the neuron fires. When the neuron fires its output is set to 1, otherwise itâ€™s set to 0.\n",
    "\n",
    "The equation is as follows:\n",
    "$$h(x) = \\left\\{\\begin{matrix}\n",
    "1;\\ if\\ w_{1}\\cdot x_{1} + w_{2}\\cdot x_{2} + w_{3}\\cdot x_{3}...w_{d}\\cdot x_{d} \\geq \\Theta  \\\\\n",
    "0;\\ if\\ w_{1}\\cdot x_{1} + w_{2}\\cdot x_{2} + w_{3}\\cdot x_{3}...w_{d}\\cdot x_{d} < \\Theta\n",
    "\\end{matrix}\\right.$$\n",
    "\n",
    "The equation can be re-written as follows including what it's known as the bias term: $$x_{0} = 1, w_{0} = \\theta$$\n",
    "\n",
    "$$h(x) = \\left\\{\\begin{matrix}\n",
    "1;\\ if\\ w_{0}\\cdot x_{0} + w_{1}\\cdot x_{1} + w_{2}\\cdot x_{2}...w_{d}\\cdot x_{d} \\geq 0  \\\\\n",
    "0;\\ if\\ w_{0}\\cdot x_{0} + w_{1}\\cdot x_{1} + w_{2}\\cdot x_{2}...w_{d}\\cdot x_{d} < 0\n",
    "\\end{matrix}\\right.$$\n",
    "\n",
    "$$h(x) = \\left\\{\\begin{matrix}\n",
    "1;\\ if\\ w^{t}\\cdot x\\geq 0 \\\\\n",
    "0;\\ if\\ w^{t}\\cdot x < 0\n",
    "\\end{matrix}\\right.$$\n",
    "\n",
    "![Rosenblatt perceptron](https://sebastianraschka.com/images/blog/2015/singlelayer_neural_networks_files/perceptron_schematic.png)\n",
    "\n",
    "This model implements the functioning of a single neuron that can solve linear classification problems through very simple learning algorithms. Rosenblatt Perceptrons are considered as the first generation of neural networks (the network is only compound of one neuron â˜º). This simple single neuron model has the main limitation of not being able to solve non-linear separable problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9f25bc",
   "metadata": {},
   "source": [
    "## 3.2\tUse a simple perceptron with weights w0, w1, and w2 as âˆ’1, 2, and 1, respectively, to classify data points (3, 4); (5, 2); (1, âˆ’3); (âˆ’8, âˆ’3); (âˆ’3, 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ece32114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3\n",
      "5\n",
      "10\n",
      "output: 1\n"
     ]
    }
   ],
   "source": [
    "x_input = [3,4,5,2,1,-3,-8,-3,-3,0]\n",
    "w_weights = [-1,2,1]\n",
    "threshold = 0.5\n",
    "\n",
    "def step_function(weighted_sum):\n",
    "    if weighted_sum > threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def perceptron():\n",
    "    weighted_sum = 0\n",
    "    for x, w in zip(x_input, w_weights):\n",
    "        weighted_sum += x * w\n",
    "        print(weighted_sum)\n",
    "    return step_function(weighted_sum)\n",
    "\n",
    "\n",
    "output = perceptron()\n",
    "print(\"output: \" + str(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9c3112d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-8</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x  y\n",
       "0  3  4\n",
       "1  5  2\n",
       "2  1 -3\n",
       "3 -8 -3\n",
       "4 -3  0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "datapoints = {\"x\" : [3,5,1,-8,-3], \"y\" : [4,2,-3,-3,0]}\n",
    "data = pd.DataFrame(datapoints)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54f7d6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e7547ad250>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.plot(kind = \"scatter\", x = \"x\", y = \"y\", s=50, cmap=\"winter\")\n",
    "plt.axhline(y=0, color=\"black\", linestyle=\"--\", linewidth=2)\n",
    "plt.axvline(x=0, color=\"black\", linestyle=\"--\", linewidth=2)\n",
    "x = np.linspace(0, 10) # >>> 50 \n",
    "y = 1.5 - 1*np.linspace(0, 10) # >>> 50 \n",
    "\n",
    "plt.plot(x, y, \"r--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c93ee2",
   "metadata": {},
   "source": [
    "This problem is not solvable by simple perceptron and i am not finding anything to plot a best fit line here so that all of the points would get classified. Also no target value is there. so, its not possible using this architecture. plotting is done here but not by the output of Perceptron as not output or Label is specified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56730a72",
   "metadata": {},
   "source": [
    "## 4.\tExplain the basic structure of a multi-layer perceptron. Explain how it can solve the XOR problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4d0803",
   "metadata": {},
   "source": [
    "A Perceptron is a linear function that produces a straight line. So to fit a non-linear data say it is in the shape of a triangle we need three perceptrons. So, we can design a neural network which has more than one neuron. We could find many different complex dataset for practice purpose on [Tensorflow Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.26332&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false). We try to model a spiral dataset to distinguish between two classes. In order to fit this dataset, we need to build a neural network that contains tens of neurons. A very common neural network architecture is to stack the neurons in layers on top of each other, called *`hidden layers`*. Each layer has *`n`* number of neurons. Layers are connected to each other by weight connections. This leads to the multilayer perceptron (MLP) architecture as shown in the figure:\n",
    "\n",
    "![MLP](https://i.stack.imgur.com/mT4Nc.png)\n",
    "\n",
    "The main components of the neural network architecture are as follows:\n",
    "\n",
    "1. Input Layer\n",
    "2. Hidden Layers\n",
    "3. Weight connections (edges)\n",
    "4. Output Layer\n",
    "\n",
    "The solution to fitting more complex (*i.e.* non-linear) models with neural networks is to use a more complex network that consists of more than just a single perceptron. The take-home message from the perceptron is that all of the learning happens by adapting the synapse weights until prediction is satisfactory. Hence, a reasonable guess at how to make a perceptron more complex is to simply **add more weights**.\n",
    "\n",
    "There are two ways to add complexity:\n",
    "\n",
    "1. Add backward connections, so that output neurons feed back to input nodes, resulting in a **recurrent network**\n",
    "2. Add neurons between the input nodes and the outputs, creating an additional (\"hidden\") layer to the network, resulting in a **multi-layer perceptron**\n",
    "\n",
    "The latter approach is more common in applications of neural networks.\n",
    "\n",
    "![multilayer](http://d.pr/i/14BS1+)\n",
    "\n",
    "How to train a multilayer network is not intuitive. Propagating the inputs forward over two layers is straightforward, since the outputs from the hidden layer can be used as inputs for the output layer. However, the process for updating the weights based on the prediction error is less clear, since it is difficult to know whether to change the weights on the input layer or on the hidden layer in order to improve the prediction.\n",
    "\n",
    "Updating a multi-layer perceptron (MLP) is a matter of: \n",
    "\n",
    "1. moving forward through the network, calculating outputs given inputs and current weight estimates\n",
    "2. moving backward updating weights according to the resulting error from forward propagation. \n",
    "\n",
    "**In this sense, it is similar to a single-layer perceptron, except it has to be done twice, once for each layer and it solves the XOR problem using these hidden layers that is many perceptrons at the same time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07499d9e",
   "metadata": {},
   "source": [
    "## 5.\tWhat is artificial neural network (ANN)? Explain some of the salient highlights in the different architectural options for ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eff2c8",
   "metadata": {},
   "source": [
    "Artificial neural networks (ANN) are basically an `*information processing system*` with highly interconnected processing elements or units that simulates the behavioural aspects of a biological neural network or a brain with certain assumptions. It can be also put forth as a mathematical model inspired by biological neural network.Robert Hecht-Nielsen, an adjunct professor of electrical and computer engineering at the University of California, San Diago in 2005, and also the inventor of one of the first neuro-computer, had proposed the fundamental mechanism of thought process called `*confabulation theory*`. He defines neural network as a 'computing system made up a number of simple, highly interconnected processing elements which process information by their dynamic state response to external inputs'.\n",
    "\n",
    "![ANNs](https://i.postimg.cc/pLgLsJDt/Architecture.jpg)\n",
    "\n",
    "**Some Salient highlights in ANNs:**\n",
    "\n",
    "The development of ANNs dates back to the early twentieth century. The major classification of NN are into three types: First-generation ANNs, Second generation ANNs, and Third-generation ANNs. \n",
    "\n",
    "1. In First-generation ANNs: McCulloh-Pitts threshold neurons (1943)\n",
    "2. Second-generation ANNs: Perceptron NN, Back Propagation NN, Kohonnen NN, Radial basis NN, Adaptive resonance theory, Hopfield NN, Support vector machines\n",
    "3. Third-generation ANNs: Spiking or pulsed NN\n",
    "\n",
    "![classification of ANNs](https://ars.els-cdn.com/content/image/1-s2.0-S2405844018332067-gr3.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd8ba32",
   "metadata": {},
   "source": [
    "## 6.\tExplain the learning process of an ANN. Explain, with example, the challenge in assigning synaptic weights for the interconnection between neurons? How can this challenge be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a8616a",
   "metadata": {},
   "source": [
    "It is a common saying that 'practice makes a man perfect'. Therefore, for any system to be perfect, it needs to learn and get trained. Learning can happen through interactions between two entities such as a system interacting with an environment. The way an ANN learns can be classified as follows:\n",
    "\n",
    "1. Supervised Learning\n",
    "2. Unsupervised Learning\n",
    "3. Reinforced Learning\n",
    "\n",
    "Suppose we are training a neural network to classify using any one of these Learning methods. In first NN it was difficult to train the network for performing a task which is not linear, so several different architectures were proposed with the base being Supervised, Unsupervised and Reinforced Learning methods. In any NN synaptic weights are the most important as they decide what the network will learn and how well it will learn. In first generation NN the weights were so simple chosen by simply using random weights and then multiplying the weights with the inputs. Then in second generation many new concepts came into picture which helped in classifying a pattern which in not linear in nature, e.g., a spiral pattern, XOR logic gate data points, and many more. It was a `Challenge` in the first generation as the concept of backpropagation, hidden layers, non-linear activation functions was not there. But in the second generation all these helped in the challeneg of finding out the optimized weights by which a NN learns about the relationship of the dataset so well and not only learns but it was able to generalize it as well. In third generation NN, Spiking neural networks, also called 'pulsed neural network', differentiates itself from the second generation neural networks where the information is encoded and decoded by a series of pulses (i.e., action potiential). Spiking neuron models can be classified into three main classes: (i) threshold fire, (ii) conductance-based , and (iii) compartmental models. Spike response NN model is used to forecast electrical load and is presented to validate its engineering application. This is a area of active research have a bright future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f03ed4",
   "metadata": {},
   "source": [
    "## 7.\tExplain, in details, the backpropagation algorithm. What are the limitations of this algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763caf78",
   "metadata": {},
   "source": [
    "* Backpropagation is a learning procedure for neurons.\n",
    "* Backpropagation repeatedly adjusts weights of the connections (weights) in the network to minimize the cost function (the difference between the actual output vector and the desired output vector).\n",
    "* As a result of the weight adjustments, hidden layers come to represent important features other than the features represented int the input layer.\n",
    "* For each layer, the goal is to find a set of weights that ensures that for each input vector, the output vector produced is the same as (or close to) the desired output vector. The difference in values between the produced and desired outputs is called the error function.\n",
    "* The backward pass (backpropagation) starts at the end of the network, backpropagates or feeds the error back, recursively applies the chain rule to compute gradients all the way to the inputs of the network, and then update the weights.\n",
    "* To reiterate, the goal of a typical neural network problem is to discover a model that best fits our data. Ultimately, we want to minimize the cost or loss function by choosing the best set of weight parameters.\n",
    "\n",
    "![Forward and Backward pass](https://i0.wp.com/neptune.ai/wp-content/uploads/Backpropagation-passes-architecture.png?resize=434%2C414&ssl=1)\n",
    "\n",
    "**Some limitations of Backpropagation algorithms are:**\n",
    "* It relies on input to perform on a specific problem.\n",
    "* Sensitive to complex/noisy data.\n",
    "* It needs the derivatives of activation functions for the network design time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3198513f",
   "metadata": {},
   "source": [
    "## 8.\tDescribe, in details, the process of adjusting the interconnection weights in a multi-layer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666a1fd0",
   "metadata": {},
   "source": [
    "Backpropagation might sound clearer when we have only one weight. We simply adjust the weight by adding the $\\Delta$w to the old weight $w_{new}$ = w - $\\alpha$$\\frac{dE}{dW}$.\n",
    "But it gets complicated when we have a multilayer perceptron (MLP) network with many weight variables.\n",
    "\n",
    "![MLP](https://miro.medium.com/proxy/1*96WV4tWwjLMGdxaKIQuADg.png)\n",
    "\n",
    "We use Chain rule for calculation of the gradients and update all the weights.\n",
    "The chain rule is a way to compute the derivative of a function whose variables are themselves functions of other variables.  If $C$ is a scalar-valued function of a scalar $z$ and $z$ is itself a scalar-valued function of another scalar variable $w$, then the chain rule states that\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w} = \\frac{\\partial C}{\\partial z}\\frac{\\partial z}{\\partial w}\n",
    "$$\n",
    "For scalar-valued functions of more than one variable, the chain rule essentially becomes additive.  In other words, if $C$ is a scalar-valued function of $N$ variables $z_1, \\ldots, z_N$, each of which is a function of some variable $w$, the chain rule states that\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w} = \\sum_{i = 1}^N \\frac{\\partial C}{\\partial z_i}\\frac{\\partial z_i}{\\partial w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9617e76",
   "metadata": {},
   "source": [
    "## 9.\tWhat are the steps in the backpropagation algorithm? Why a multi-layer neural network is required?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acab12b",
   "metadata": {},
   "source": [
    "Backpropagation is the core of how neural networks learn. Training a neural network typically happens by the repetition of the following three steps:\n",
    "\n",
    "* Feedforward: get the linear combination (weighted sum), and apply the activation function to get the output prediction ($\\hat{y}$):\n",
    "\n",
    "$$\\hat{y} = \\sigma \\cdot W^{(3)}\\cdot \\sigma \\cdot W^{(2)} \\cdot \\sigma \\cdot W^{(1)}\\cdot (x)$$\n",
    "\n",
    "* Compare the prediction with the label to calculate the error or loss function:\n",
    "\n",
    "$$E(W,b) = \\frac{1}{N}\\sum_{i = 1}^{N}\\left | \\hat{y_{i}}-y_{i} \\right |$$\n",
    "\n",
    "* Use a gradient descent optimization algorithm to compute the $\\Delta$w that optimizes the error function:\n",
    "\n",
    "$$\\Delta w_{i} = -\\alpha \\frac{dE}{dw_{i}}$$\n",
    "\n",
    "Backpropagate the $\\Delta$w through the network to update the weights:\n",
    "\n",
    "$$W_{new} = W_{old} - \\alpha \\left ( \\frac{\\partial Error}{\\partial W_{x}} \\right )$$\n",
    "\n",
    "\n",
    "*`Multilayer networks solve the classification problem for non linear sets by employing hidden layers, whose neurons are not directly connected to the output. The additional hidden layers can be interpreted geometrically as additional hyper-planes, which enhance the separation capacity of the network.`*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d159a7",
   "metadata": {},
   "source": [
    "## 10.\tWrite short notes on:\n",
    "1.\tArtificial neuron\n",
    "2.\tMulti-layer perceptron\n",
    "3.\tDeep learning\n",
    "4.\tLearning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50671815",
   "metadata": {},
   "source": [
    "**1. Artificial Neuron**\n",
    "\n",
    "The development of ANNs dates back to the early twentieth century. The major classification of NN is First-generation, Second-generation and Third-generation. The initial work of Warren McCulloh and Walter Pitts in a research paper that model a simple NN with electronic circuits paved the way for a new technological revolution of NN. Such NNs constitute the first-generation NNs. Afterwards, as computers emerged in the 1950s, several researchers attempted to make use of the new technology to construct enhanced NNs. After initial stage, Single layer Perceptron, then Multi-layer Perceptron with some new techniques like Backpropagation of errors so that non linear relationship between some dataset could be learned. Some basic networks like Hopfield NN, Kohonnen NN, SVM come into picture in the second generation and in the third generation networks Spiking Neural Networks (SNN) or pulsed NN come into picture which have a great closeness to real biological neurons in a neural simulation.\n",
    "\n",
    "**2. Multi-layer perceptron**\n",
    "\n",
    "We saw that a single perceptron works great with simple datasets that can be separated by a line. But, as we can imagine, the real world is much more complex than that. As we know a Perceptron creates a single line for splitting the linear data, now to split a non-linear dataset, we need more than one line. This means we need to come up with an architecture to use tens and hundreds of neurons in our neural network. We need to remember that if we separate data which is having a shape of a triangle in between a data, so to classify we may need three perceptrons which will be three straight lines to create a triangle-like shape that splits the dataset in between. It consists of Input Layer, Hidden Layer, Weight connections (edges) and output layer.\n",
    "\n",
    "**3. Deep learning**\n",
    "\n",
    "Deep learning is a specific subfield of machine learning: a new take on learning representations from data that puts an emphasis on learning successive layers of increasingly meaningful representations. The `\"deep\"` in \"deep learning\" isn't a reference to any kind of deeper understanding achieved by the approach; rather, it stands for this idea of successive layers of representations. How many layers contribute to a model of the data is called the *`depth`* of the model. Other appropriate names for the field could have been *`layered representations learning or hierarchical representations learning.`* Modern deep learning often involves tens or even hundreds of successive layers of representations, and they are all learned automatically from exposure to training data. Meanwhile, other approaches to machine learning tend to focus on learning only one or two layers of representations of the data (say, taking a pixel histogram and then applying a classification rule); hence, they are sometimes called shallow learning. In deep learning, these layered representations are learned via models called neural networks, structured in literal layers stacked on top of each other.\n",
    "\n",
    "**4. Learning Rate**\n",
    "\n",
    "Learning Rate is a hyperparameter which is set by us during the training of a network. If it is value is too low the training will be slow and may be not reach the local minima, and if it is too large the gradient of the network could overshot and never reach the local minima. The range may lie between 0.00001 to 1. It is not pre-defined anywhere and lies on the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e29fc10",
   "metadata": {},
   "source": [
    "## 11.\tWrite the difference between:-\n",
    "1.\tActivation function vs threshold function\n",
    "2.\tStep function vs sigmoid function\n",
    "3.\tSingle layer vs multi-layer perceptron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e314356e",
   "metadata": {},
   "source": [
    "**1. Activation function vs threshold function**\n",
    "\n",
    "Binary Step function is a threshold based activation function, in which any value say 0.5, if some value is greater than 0.5 it will be 1 and if less than 0.5, the value will be zero. It helps us in binary class classification. It is used for classification problems like true or false, spam or not spam, and pass or fail.\n",
    "\n",
    "Whereas sigmoid, ReLU, LeakyReLU are activation functions are non-linear. Deep learning practitioners today work with data of high dimensionality such as images, audios, videos, etc. With the drawbacks mentioned above, it is not practical to use Linear Activation Functions in complex applications that we use neural networks for. Therefore, it is Non-Linear Functions that are being widely used in present.\n",
    "\n",
    "**2. Step function vs sigmoid function**\n",
    "\n",
    "Step function produces a binary output. It basically says that if the input > 0, it fires (output y = 1); else (input < 0); it doesn't fire (output y = 0). It is mainly used in classification problems like true or false, spam or not spam, and pass or fail. Whereas, in case of sigmoid is often used in binary classifiers to predict the *`probability`* of a class when we have two classes. The sigmoid squishes all the values to a probability between 0 and 1, which reduces extreme values or outliers in the data without removing them.\n",
    "\n",
    "**3. Single layer vs multi-layer perceptron**\n",
    "\n",
    "The most simple neural network is the perceptron, which consists of a single neuron. Conceptually, the perceptron functions in a manner similar to a biological neuron. It simply creates a single best fit line, it performs two consecutive functions; it calculates the *`weighted sum`* of the inputs to represent the total strength of the input signals, and it applies a *`step function`* to the result to determine whether to fire the output 1 if the signal exceeds a certain threshold or 0 if the signal doesn't exceed the threshold.\n",
    "\n",
    "We saw that a single perceptron works great with simple datasets that can be separated by a line. But, as we can imagine, the real world is much more complex than that. As we know a Perceptron creates a single line for splitting the linear data, now to split a non-linear dataset, we need more than one line. This means we need to come up with an architecture to use tens and hundreds of neurons in our neural network. We need to remember that if we separate data which is having a shape of a triangle in between a data, so to classify we may need three perceptrons which will be three straight lines to create a triangle-like shape that splits the dataset in between. It consists of Input Layer, Hidden Layer, Weight connections (edges) and output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9038f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
